{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './input/mfcc_train.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3b23502baaef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m     \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_mfcc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverified\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_mfcc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-3b23502baaef>\u001b[0m in \u001b[0;36mextract_mfcc\u001b[1;34m(self, train)\u001b[0m\n\u001b[0;32m    120\u001b[0m             })\n\u001b[0;32m    121\u001b[0m             \u001b[0mdf_mfcc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'mfcc_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./input/mfcc_train.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m                 \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmfccs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './input/mfcc_train.p'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, scale\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self):\n",
    "        # Audio parameters\n",
    "        self.audio_length = 2\n",
    "        self.sampling_rate = 44100\n",
    "        step = 512  # numbers of point librosa takes to create 20 features\n",
    "        self.time_steps = self.audio_length * self.sampling_rate // step\n",
    "\n",
    "        # RNN parameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.display_step = 50\n",
    "        self.test_step = 200\n",
    "        self.nb_epochs = 2000\n",
    "        self.batch_size = 64\n",
    "        self.n_features = 20\n",
    "        self.rnn_sizes = [128, 128]\n",
    "        self.model_name = 'mfcc_cross'\n",
    "        self.load_model_name = 'mfcc_cross'\n",
    "        self.load = True\n",
    "\n",
    "        self.export_dir = './saved_models/'\n",
    "        self.import_dir = './'\n",
    "        self.label_file = 'dataset_minor.csv'\n",
    "        self._init_encoders()\n",
    "\n",
    "    def _init_encoders(self):\n",
    "        df = pd.read_csv(self.import_dir + self.label_file)\n",
    "\n",
    "        # Encode string label to int\n",
    "        self.enc = LabelEncoder()\n",
    "        self.enc.fit(df.corona_test)\n",
    "        self.n_classes = len(self.enc.classes_)\n",
    "\n",
    "        # Encode int label to one hot [0, 0, 1, 0, 0]\n",
    "        self.enc_hot = OneHotEncoder()\n",
    "        self.enc_hot.fit(self.enc.transform(df.corona_test).reshape(-1, 1))\n",
    "\n",
    "    def extract_mfcc(self, train=True):\n",
    "        \"\"\"\n",
    "        Split the audio files in segments of 2 seconds, and compute the mfcc\n",
    "        features for all of the\n",
    "        :param train: True for train set, False for test set\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            df = pd.read_csv(self.import_dir + self.label_file)\n",
    "            df = df.set_index('cough_filename')\n",
    "            labels = []\n",
    "            verifieds = []\n",
    "            path = 'rnn_data/train'\n",
    "        else:\n",
    "            path = 'rnn_data/test'\n",
    "\n",
    "        mfccs = []\n",
    "        fnames = []\n",
    "        count = 0\n",
    "\n",
    "        for fn in glob.glob(\n",
    "                os.path.join(self.import_dir + path, '*.wav')):\n",
    "            fname = fn.split('/')[-1]\n",
    "            if train:\n",
    "                label = df.loc[cough_filename].corona_test\n",
    "                \n",
    "            sound_clip, s = librosa.load(fn, sr=self.sampling_rate)\n",
    "\n",
    "            try:\n",
    "                mfcc = librosa.feature.mfcc(y=sound_clip, sr=s,\n",
    "                                            n_mfcc=self.n_features).T\n",
    "            except ValueError:\n",
    "                # Some files of the test set cause problems\n",
    "                mfcc = np.ones((10, self.n_features))\n",
    "            time_steps, _ = mfcc.shape\n",
    "\n",
    "            mfcc = scale(mfcc, axis=0)\n",
    "\n",
    "            pad = self.time_steps - time_steps % self.time_steps\n",
    "\n",
    "            # pad with zeros to have the same time_steps\n",
    "            if pad < self.time_steps // 3 or time_steps // self.time_steps == 0:\n",
    "                mfcc = np.pad(mfcc, ((0, pad), (0, 0)), mode='constant',\n",
    "                              constant_values=(0, 0))\n",
    "                mfcc = mfcc.reshape(time_steps // self.time_steps + 1,\n",
    "                                    self.time_steps, self.n_features)\n",
    "\n",
    "            # remove the last part if it is too short\n",
    "            else:\n",
    "                mfcc = mfcc[:time_steps // self.time_steps * self.time_steps, :]\n",
    "                mfcc = mfcc.reshape(time_steps // self.time_steps,\n",
    "                                    self.time_steps,\n",
    "                                    self.n_features)\n",
    "\n",
    "            for i in range(mfcc.shape[0]):\n",
    "                mfccs.append(mfcc[i, :, :])\n",
    "                fnames.append(fname)\n",
    "                if train:\n",
    "                    labels.append(label)\n",
    "                    \n",
    "\n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                print(\"file {}\".format(count))\n",
    "\n",
    "        if train:\n",
    "            df_mfcc = pd.DataFrame({\n",
    "                'fname': fnames,\n",
    "                'label': labels,\n",
    "                \n",
    "            })\n",
    "            df_mfcc.to_csv(self.import_dir + 'mfcc_train.csv')\n",
    "            with open('./input/mfcc_train.p', 'wb') as fp:\n",
    "                pickle.dump(np.array(mfccs), fp)\n",
    "        else:\n",
    "            df_mfcc = pd.DataFrame({\n",
    "                'fname': fnames,\n",
    "            })\n",
    "            df_mfcc.to_csv(self.import_dir + 'mfcc_test.csv')\n",
    "            with open('./input/mfcc_test.p', 'wb') as fp:\n",
    "                pickle.dump(np.array(mfccs), fp)\n",
    "\n",
    "    def train(self, verified=False):\n",
    "        \"\"\"\n",
    "        Train on the mfcc features\n",
    "        \"\"\"\n",
    "\n",
    "        with open('./input/mfcc_train.p', 'rb') as fp:\n",
    "            X = pickle.load(fp)\n",
    "\n",
    "        df_mfcc = pd.read_csv(self.import_dir + 'mfcc_train.csv')\n",
    "\n",
    "        label = self.enc_hot.transform(\n",
    "            self.enc.transform(df_mfcc.label).reshape(-1, 1)).toarray()\n",
    "\n",
    "        # take only the manually verified label\n",
    "       \n",
    "            \n",
    "        idx_verif = df_mfcc.index\n",
    "        X = X[idx_verif]\n",
    "        label = label[idx_verif]\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "        for i, index in enumerate(kf.split(X)):\n",
    "            train_index, test_index = index\n",
    "\n",
    "            X_train = X[train_index, :, :]\n",
    "            X_test = X[test_index, :, :]\n",
    "\n",
    "            y_train = label[train_index]\n",
    "            y_test = label[test_index]\n",
    "\n",
    "            # RNN\n",
    "            tf.reset_default_graph()\n",
    "\n",
    "            x = tf.placeholder(\"float\", [None, self.time_steps, self.n_features])\n",
    "            y = tf.placeholder(\"float\", [None, self.n_classes])\n",
    "            keep_prob = tf.placeholder(\"float\", name='keep_prob')\n",
    "\n",
    "            prediction = self.build_rnn(x, keep_prob)\n",
    "\n",
    "            # Define loss and optimizer\n",
    "            loss_f = -tf.reduce_sum(y * tf.log(prediction + 1e-10))\n",
    "            optimizer = tf.train.RMSPropOptimizer(\n",
    "                learning_rate=self.learning_rate).minimize(loss_f)\n",
    "\n",
    "            # Evaluate model\n",
    "            correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "            # Initializing the variables\n",
    "            init = tf.global_variables_initializer()\n",
    "            with tf.Session() as session:\n",
    "                saver = tf.train.Saver()\n",
    "                if self.load:\n",
    "                    saver.restore(session, self.export_dir + self.load_model_name + '{}'.format(i))\n",
    "                else:\n",
    "                    session.run(init)\n",
    "\n",
    "                t0 = time.time()\n",
    "                for epoch in range(self.nb_epochs):\n",
    "                    start = epoch * self.batch_size % (len(y_train) - self.batch_size)\n",
    "                    batch_x = X_train[start:start + self.batch_size, :, :]\n",
    "                    batch_y = y_train[start:start + self.batch_size]\n",
    "\n",
    "                    _, c = session.run([optimizer, loss_f], feed_dict={x: batch_x, y: batch_y, keep_prob: 0.7})\n",
    "\n",
    "                    if epoch % self.display_step == 0:\n",
    "                        # Calculate batch accuracy\n",
    "                        acc = session.run(accuracy, feed_dict={x: batch_x, y: batch_y, keep_prob: 1})\n",
    "                        # Calculate batch loss\n",
    "                        loss = session.run(loss_f, feed_dict={x: batch_x, y: batch_y, keep_prob: 1})\n",
    "\n",
    "                        print(\"Fold \" + str(i) + \", Iter \" + str(epoch) + \" / \" + str(\n",
    "                            self.nb_epochs) + \", Minibatch Loss= \" +\n",
    "                              \"{:.6f}\".format(loss) + \", Training Accuracy= \" +\n",
    "                              \"{:.5f}\".format(acc))\n",
    "\n",
    "                        print('{} epochs time: {}'.format(self.display_step, time.time() - t0))\n",
    "                        t0 = time.time()\n",
    "                    if epoch % self.test_step == 0:\n",
    "                        print('Test accuracy: ',\n",
    "                              round(session.run(accuracy, feed_dict={x: X_test, y: y_test, keep_prob: 1}), 3))\n",
    "\n",
    "                saver.save(session, self.export_dir + self.model_name + '{}'.format(i))\n",
    "\n",
    "    def build_rnn(self, x, keep_prob):\n",
    "        \"\"\"\n",
    "        Build the network\n",
    "        \"\"\"\n",
    "        layer = {\n",
    "            'weight': tf.Variable(\n",
    "                tf.truncated_normal([self.rnn_sizes[-1], self.n_classes],\n",
    "                                    stddev=0.01)),\n",
    "            'bias': tf.Variable(tf.constant(0.1, shape=[self.n_classes]))}\n",
    "        lstm_cells = [rnn_cell.LSTMCell(rnn_size) for rnn_size in\n",
    "                      self.rnn_sizes]\n",
    "        drop_cells = [\n",
    "            tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) for\n",
    "            lstm in lstm_cells]\n",
    "\n",
    "        lstm = rnn_cell.MultiRNNCell(drop_cells)\n",
    "        output, state = tf.nn.dynamic_rnn(lstm, x, dtype=tf.float32,\n",
    "                                          sequence_length=self.length(x))\n",
    "        last = self.last_relevant(output, self.length(x))\n",
    "\n",
    "        return tf.nn.softmax(\n",
    "            tf.tensordot(last, layer['weight'], [[1], [0]]) + layer[\n",
    "                'bias'])\n",
    "\n",
    "    @staticmethod\n",
    "    def length(sequence):\n",
    "        \"\"\"\n",
    "        From https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "        \"\"\"\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "        length = tf.reduce_sum(used, 1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "\n",
    "    @staticmethod\n",
    "    def last_relevant(output, length):\n",
    "        \"\"\"\n",
    "        Return the last relevant output of the LSTM cell, by removing the\n",
    "        trailing zeros (!! Raise an error it the array is full of zeros)\n",
    "        From https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(output)[0]\n",
    "        max_length = tf.shape(output)[1]\n",
    "        out_size = int(output.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "        flat = tf.reshape(output, [-1, out_size])\n",
    "        relevant = tf.gather(flat, index)\n",
    "        return relevant\n",
    "\n",
    "    def predict(self):\n",
    "        with open('./input/mfcc_test.p', 'rb') as fp:\n",
    "            X = pickle.load(fp)\n",
    "\n",
    "        df_mfcc = pd.read_csv(self.import_dir + 'mfcc_test.csv')\n",
    "        predictions = np.zeros((5, len(pd.unique(df_mfcc.fname)), self.n_classes))\n",
    "        for j in range(5):\n",
    "\n",
    "            # RNN\n",
    "            tf.reset_default_graph()\n",
    "\n",
    "            # variables\n",
    "            x = tf.placeholder(\"float\", [None, self.time_steps, self.n_features])\n",
    "            keep_prob = tf.placeholder(\"float\", name='keep_prob')\n",
    "\n",
    "            prediction = self.build_rnn(x, keep_prob)\n",
    "\n",
    "            with tf.Session() as session:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.restore(session, self.export_dir + self.model_name + '{}'.format(j))\n",
    "                unique = pd.unique(df_mfcc.fname)\n",
    "                for i in range(len(pd.unique(df_mfcc.fname))):\n",
    "                    idxs = df_mfcc.fname[df_mfcc.fname == unique[i]].index.tolist()\n",
    "\n",
    "                    batch = X[idxs, :, :]\n",
    "                    if batch.sum() == 0:\n",
    "                        batch = np.ones_like(batch)\n",
    "\n",
    "                    pred = session.run(prediction, feed_dict={x: np.array(batch), keep_prob: 1})\n",
    "                    # Average on the segment for the same audio\n",
    "                    predictions[j, i, :] = pred.mean(axis=0)\n",
    "            print(\"Network {} done ! \".format(j))\n",
    "        unique = pd.unique(df_mfcc.fname)\n",
    "        results = {'label': [], 'fname': []}\n",
    "        # Average on the 5 networks\n",
    "        predictions = predictions.mean(axis=0)\n",
    "        for i in range(len(pd.unique(df_mfcc.fname))):\n",
    "            top3_labels = self.top_3(predictions[i, :], return_string=True)\n",
    "            results['label'].append(top3_labels)\n",
    "            results['fname'].append(unique[i])\n",
    "\n",
    "            print('Label for {}: {}'.format(i, top3_labels))\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        print(df.head())\n",
    "        df.to_csv(\"output/{}.csv\".format(self.model_name), index=False)\n",
    "\n",
    "    def top_3(self, predictions, return_string=True):\n",
    "        top_labels = np.argsort(predictions)\n",
    "        top_labels = top_labels[::-1]\n",
    "        top3_labels = top_labels[:3]\n",
    "\n",
    "        if return_string:\n",
    "            top3_labels = \" \".join([self.enc.inverse_transform(el) for el in top3_labels])\n",
    "        return top3_labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    rnn = RNN()\n",
    "    rnn.extract_mfcc()\n",
    "    rnn.train(verified=False)\n",
    "    rnn.extract_mfcc(train=False)\n",
    "    rnn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
